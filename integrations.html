<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>API Integrations - Airtable Lead Enricher</title>
    <link rel="icon" type="image/png" href="https://images.apifyusercontent.com/hIWSIagcj8949hoLcWRj_q-7YXhT9xpVuYWR4RP93uU/rs:fill:250:250/cb:1/aHR0cHM6Ly9hcGlmeS1pbWFnZS11cGxvYWRzLXByb2QuczMudXMtZWFzdC0xLmFtYXpvbmF3cy5jb20vOHp1MThIQW4xN1Q4NXpGeFUtYWN0b3ItY1ZEcEF0dHBoY3BETTR2RngtMnpveEVnMEhlRi1haXJ0YWJsZS1sZWFkLWVucmljaGVyLXRyYW5zcGFyZW50LnBuZw.webp">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }

        .doc-layout {
            display: flex;
            min-height: 100vh;
        }

        /* Sidebar TOC */
        .sidebar {
            width: 280px;
            background: white;
            border-right: 1px solid #e0e0e0;
            position: fixed;
            height: 100vh;
            overflow-y: auto;
            padding: 20px;
        }

        .sidebar-header {
            margin-bottom: 20px;
            padding-bottom: 15px;
            border-bottom: 2px solid #667eea;
        }

        .sidebar-header h2 {
            color: #667eea;
            font-size: 1.3em;
            margin-bottom: 5px;
        }

        .sidebar-header .version {
            color: #666;
            font-size: 0.85em;
        }

        .toc {
            list-style: none;
        }

        .toc li {
            margin-bottom: 5px;
        }

        .toc a {
            color: #333;
            text-decoration: none;
            display: block;
            padding: 8px 12px;
            border-radius: 4px;
            font-size: 0.95em;
            transition: all 0.2s;
        }

        .toc a:hover {
            background: #f0f0f0;
            color: #667eea;
        }

        .toc a.active {
            background: #667eea;
            color: white;
        }

        .toc .toc-subsection {
            list-style: none;
            margin-left: 15px;
            margin-top: 5px;
        }

        .toc .toc-subsection a {
            font-size: 0.85em;
            padding: 6px 10px;
        }

        .back-link {
            display: block;
            margin-bottom: 15px;
            color: #667eea;
            font-weight: 500;
            font-size: 0.9em;
        }

        /* Main content */
        .main-content {
            margin-left: 280px;
            flex: 1;
            padding: 40px 60px;
            max-width: 1200px;
        }

        .content-wrapper {
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.05);
        }

        h1 {
            color: #667eea;
            margin-bottom: 10px;
            font-size: 2.5em;
        }

        .version-tag {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 30px;
            display: inline-block;
            padding: 5px 15px;
            background: #f0f0f0;
            border-radius: 20px;
        }

        h2 {
            color: #764ba2;
            margin-top: 50px;
            margin-bottom: 20px;
            font-size: 1.8em;
            border-bottom: 2px solid #f0f0f0;
            padding-bottom: 10px;
            padding-top: 20px;
        }

        h2:first-of-type {
            margin-top: 30px;
        }

        h3 {
            color: #667eea;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        p {
            margin-bottom: 15px;
            line-height: 1.8;
        }

        ul, ol {
            margin-bottom: 15px;
            margin-left: 25px;
        }

        li {
            margin-bottom: 8px;
            line-height: 1.6;
        }

        a {
            color: #667eea;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        strong {
            color: #333;
        }

        .info-box {
            background: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .warning-box {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .success-box {
            background: #d4edda;
            border-left: 4px solid #28a745;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }

        code {
            background: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: #282c34;
            border-radius: 4px;
            overflow-x: auto;
            margin: 1rem 0;
        }

        pre code {
            background: transparent;
            padding: 0;
            display: block;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }

        th, td {
            padding: 0.75rem;
            text-align: left;
            border-bottom: 1px solid #e2e8f0;
        }

        th {
            background: #f7fafc;
            font-weight: 600;
            color: #667eea;
        }

        tr:hover {
            background: #f9f9f9;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .sidebar {
                display: none;
            }

            .main-content {
                margin-left: 0;
                padding: 20px;
            }

            .content-wrapper {
                padding: 20px;
            }

            h1 {
                font-size: 2em;
            }

            h2 {
                font-size: 1.5em;
            }
        }
    </style>
</head>
<body>
    <div class="doc-layout">
        <!-- Left Sidebar TOC -->
        <aside class="sidebar">
            <a href="index.html" class="back-link">← Documentation</a>

            <div class="sidebar-header">
                <h2>API Integrations</h2>
                <span class="version">API Mode</span>
            </div>

            <nav>
                <ul class="toc">
                    <li><a href="#overview">API Mode Overview</a></li>
                    <li><a href="#python">Python Example</a></li>
                    <li><a href="#nodejs">Node.js Example</a></li>
                    <li><a href="#output-schema">Output Schema</a></li>
                    <li><a href="#webhook">Webhook Integration</a></li>
                    <li><a href="#lambda">AWS Lambda</a></li>
                    <li><a href="#airflow">Apache Airflow</a></li>
                    <li><a href="#glue">AWS Glue</a></li>
                    <li><a href="#gcp">Google Cloud Functions</a></li>
                    <li><a href="#step-functions">AWS Step Functions</a></li>
                    <li><a href="#snowflake">Snowflake</a></li>
                    <li><a href="#redshift">Amazon Redshift</a></li>
                    <li><a href="#data-lake">Data Lake Pattern</a></li>
                    <li><a href="#rate-limits">Rate Limits & Batching</a></li>
                    <li><a href="#cost-estimation">Cost Estimation</a></li>
                </ul>
            </nav>
        </aside>

        <!-- Main Content -->
        <main class="main-content">
            <div class="content-wrapper">
                <h1 id="overview">API Integrations</h1>
                <span class="version-tag">Use without Airtable via API mode</span>

                <p>The Airtable Lead Enricher can be used as a standalone API service without requiring an Airtable base. This enables integration with any data pipeline, workflow automation, or business intelligence tool.</p>

                <h2 id="overview">API Mode Overview</h2>
                <table>
                    <thead>
                        <tr>
                            <th>Feature</th>
                            <th>Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Max companies/run</td>
                            <td>100</td>
                        </tr>
                        <tr>
                            <td>Input</td>
                            <td>JSON array</td>
                        </tr>
                        <tr>
                            <td>Output</td>
                            <td>Dataset + Webhook</td>
                        </tr>
                        <tr>
                            <td>Airtable required</td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>

                <h2 id="python">Python Example</h2>
                <p>Enrich leads using Python with both asynchronous and synchronous approaches:</p>

                <h3>Asynchronous (Recommended for Large Batches)</h3>
                <pre><code class="language-python">import requests
import time

APIFY_TOKEN = "your_token"

# Start run
run = requests.post(
    "https://api.apify.com/v2/acts/datahq~airtable-lead-enricher/runs",
    params={"token": APIFY_TOKEN},
    json={
        "mode": "api",
        "companies": [
            {"companyName": "Acme Corp", "website": "https://acme.com"}
        ],
        "enrichment": {
            "sources": ["google_maps", "website", "hunter"],
            "hunter": {
                "enabled": True,
                "apiKey": "YOUR_HUNTER_KEY"
            }
        },
        "llm": {
            "enabled": True,
            "provider": "openai",
            "apiKey": "YOUR_OPENAI_KEY"
        }
    }
).json()

# Wait for completion
run_id = run["data"]["id"]

while True:
    status = requests.get(
        f"https://api.apify.com/v2/actor-runs/{run_id}",
        params={"token": APIFY_TOKEN}
    ).json()

    if status["data"]["status"] in ["SUCCEEDED", "FAILED"]:
        break
    time.sleep(5)

# Get results
dataset_id = status["data"]["defaultDatasetId"]
results = requests.get(
    f"https://api.apify.com/v2/datasets/{dataset_id}/items",
    params={"token": APIFY_TOKEN}
).json()

print(results)</code></pre>

                <h3>Synchronous (Wait for Results)</h3>
                <pre><code class="language-python">response = requests.post(
    "https://api.apify.com/v2/acts/datahq~airtable-lead-enricher/run-sync-get-dataset-items",
    params={"token": APIFY_TOKEN},
    json={"mode": "api", "companies": [...]},
    timeout=300
)
results = response.json()</code></pre>

                <h2 id="nodejs">Node.js Example</h2>
                <p>Enrich leads using the Apify JavaScript client:</p>

                <pre><code class="language-javascript">const { ApifyClient } = require('apify-client');
const client = new ApifyClient({ token: 'your_token' });

async function enrichLeads(companies) {
    const run = await client.actor('datahq/airtable-lead-enricher').call({
        mode: 'api',
        companies: companies,
        enrichment: {
            sources: ['google_maps', 'website', 'hunter'],
            hunter: {
                enabled: true,
                apiKey: 'YOUR_HUNTER_KEY'
            }
        },
        llm: {
            enabled: true,
            provider: 'openai',
            apiKey: 'YOUR_OPENAI_KEY'
        }
    });

    const { items } = await client.dataset(run.defaultDatasetId).listItems();
    return items;
}

// Usage
const companies = [
    { companyName: 'Acme Corp', website: 'https://acme.com' }
];

enrichLeads(companies).then(results => {
    console.log(results);
});</code></pre>

                <h2 id="output-schema">Output Schema</h2>
                <p>Each enriched lead returns the following JSON structure:</p>

                <pre><code class="language-json">{
  "companyName": "Acme Corp",
  "website": "https://acme.com",
  "location": "San Francisco, CA",
  "email": "contact@acme.com",
  "phone": "+1 555 0100",
  "address": "123 Main St, San Francisco, CA",
  "rating": 4.5,
  "reviewCount": 127,
  "linkedinUrl": "https://linkedin.com/company/acme",
  "facebookUrl": "https://facebook.com/acme",
  "twitterHandle": "@acme",
  "industry": "Technology",
  "leadScore": 85,
  "icpScore": 36,
  "icpReasoning": "Strong tech stack alignment...",
  "summary": "Acme Corp is an enterprise...",
  "dataConfidence": 0.87,
  "enrichedAt": "2025-12-19T14:30:00Z",
  "enrichmentSources": ["google_maps", "website", "hunter"]
}</code></pre>

                <h2 id="webhook">Webhook Integration</h2>
                <p>Receive enrichment results via webhook for event-driven architectures:</p>

                <h3>Configure Webhook</h3>
                <pre><code class="language-json">{
  "mode": "api",
  "companies": [...],
  "webhookUrl": "https://your-system.com/webhook"
}</code></pre>

                <h3>Webhook Payload</h3>
                <p>When the enrichment completes, you'll receive:</p>

                <pre><code class="language-json">{
  "type": "RUN_COMPLETED",
  "runId": "abc123",
  "datasetId": "xyz789",
  "timestamp": "2025-12-19T14:35:00Z",
  "stats": {
    "totalCompanies": 50,
    "successful": 47,
    "failed": 3,
    "avgLeadScore": 72.4,
    "avgDataConfidence": 0.81
  },
  "results": [...]
}</code></pre>

                <h2 id="lambda">AWS Lambda Integration</h2>
                <p>Enrich leads from S3 using AWS Lambda functions:</p>

                <pre><code class="language-python">import boto3
import requests
import json
import os

s3 = boto3.client('s3')
APIFY_TOKEN = os.environ['APIFY_TOKEN']

def lambda_handler(event, context):
    # Read companies from S3
    obj = s3.get_object(Bucket='my-bucket', Key='leads/pending.json')
    companies = json.loads(obj['Body'].read())

    # Enrich via Apify (max 100 per run)
    batches = [companies[i:i+100] for i in range(0, len(companies), 100)]
    all_results = []

    for batch in batches:
        run = requests.post(
            "https://api.apify.com/v2/acts/datahq~airtable-lead-enricher/run-sync-get-dataset-items",
            params={"token": APIFY_TOKEN},
            json={
                "mode": "api",
                "companies": batch,
                "enrichment": {"sources": ["google_maps", "website"]}
            }
        ).json()
        all_results.extend(run)

    # Write enriched data back to S3
    s3.put_object(
        Bucket='my-bucket',
        Key='leads/enriched.json',
        Body=json.dumps(all_results)
    )

    return {"enriched": len(all_results)}</code></pre>

                <h2 id="airflow">Apache Airflow DAG</h2>
                <p>Schedule and orchestrate lead enrichment with Apache Airflow:</p>

                <pre><code class="language-python">from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import requests

default_args = {
    'owner': 'data-team',
    'retries': 2
}

def enrich_leads(**context):
    """Call Apify actor to enrich leads."""
    ti = context['ti']
    leads = ti.xcom_pull(task_ids='extract_leads')

    response = requests.post(
        "https://api.apify.com/v2/acts/datahq~airtable-lead-enricher/run-sync-get-dataset-items",
        params={"token": Variable.get("APIFY_TOKEN")},
        json={
            "mode": "api",
            "companies": leads,
            "enrichment": {"sources": ["google_maps", "website"]},
            "llm": {
                "enabled": True,
                "provider": "openai",
                "apiKey": Variable.get("OPENAI_API_KEY")
            }
        },
        timeout=300
    )

    return response.json()

with DAG(
    'lead_enrichment_pipeline',
    default_args=default_args,
    schedule_interval='0 2 * * *',  # Daily at 2 AM
    start_date=datetime(2025, 1, 1),
    catchup=False
) as dag:

    extract = PythonOperator(
        task_id='extract_leads',
        python_callable=extract_leads
    )

    enrich = PythonOperator(
        task_id='enrich_leads',
        python_callable=enrich_leads
    )

    load = PythonOperator(
        task_id='load_to_warehouse',
        python_callable=load_to_warehouse
    )

    extract >> enrich >> load</code></pre>

                <h2 id="glue">AWS Glue Integration</h2>
                <p>Use AWS Glue ETL jobs to enrich data from your data catalog:</p>

                <pre><code class="language-python">import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
import requests
import json

args = getResolvedOptions(sys.argv, ['JOB_NAME', 'APIFY_TOKEN'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Read leads from S3
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="leads_db",
    table_name="raw_leads"
)

# Convert to list of companies
companies = []
for record in datasource.toDF().collect():
    companies.append({
        "companyName": record.company_name,
        "website": record.website
    })

# Enrich in batches (max 100 per run)
enriched_results = []
for i in range(0, len(companies), 100):
    batch = companies[i:i+100]

    response = requests.post(
        "https://api.apify.com/v2/acts/datahq~airtable-lead-enricher/run-sync-get-dataset-items",
        params={"token": args['APIFY_TOKEN']},
        json={
            "mode": "api",
            "companies": batch,
            "enrichment": {"sources": ["google_maps", "website"]}
        },
        timeout=300
    )
    enriched_results.extend(response.json())

# Write enriched data back to S3
df = spark.createDataFrame(enriched_results)
glueContext.write_dynamic_frame.from_options(
    frame=DynamicFrame.fromDF(df, glueContext, "enriched"),
    connection_type="s3",
    connection_options={"path": "s3://my-bucket/enriched-leads/"},
    format="parquet"
)

job.commit()</code></pre>

                <div class="info-box">
                    <strong>Tip:</strong> Use Glue job parameters to securely pass APIFY_TOKEN instead of hardcoding.
                </div>

                <h2 id="gcp">Google Cloud Functions</h2>
                <p>Deploy serverless enrichment functions on Google Cloud Platform:</p>

                <pre><code class="language-python">import functions_framework
from google.cloud import storage
import requests
import json
import os

@functions_framework.http
def enrich_leads(request):
    """HTTP Cloud Function to enrich leads from Cloud Storage."""

    storage_client = storage.Client()
    bucket = storage_client.bucket('my-leads-bucket')

    # Read pending leads
    blob = bucket.blob('leads/pending.json')
    companies = json.loads(blob.download_as_text())

    # Enrich via Apify
    enriched = []
    for i in range(0, len(companies), 100):
        batch = companies[i:i+100]

        response = requests.post(
            "https://api.apify.com/v2/acts/datahq~airtable-lead-enricher/run-sync-get-dataset-items",
            params={"token": os.environ['APIFY_TOKEN']},
            json={
                "mode": "api",
                "companies": batch,
                "enrichment": {"sources": ["google_maps", "website"]},
                "llm": {
                    "enabled": True,
                    "provider": "openai",
                    "apiKey": os.environ['OPENAI_API_KEY']
                }
            },
            timeout=300
        )
        enriched.extend(response.json())

    # Write enriched data
    output_blob = bucket.blob('leads/enriched.json')
    output_blob.upload_from_string(
        json.dumps(enriched, indent=2),
        content_type='application/json'
    )

    return {
        "success": True,
        "enriched": len(enriched),
        "output": "gs://my-leads-bucket/leads/enriched.json"
    }</code></pre>

                <h3>Deploy</h3>
                <pre><code class="language-bash">gcloud functions deploy enrich-leads \
  --runtime python39 \
  --trigger-http \
  --allow-unauthenticated \
  --set-env-vars APIFY_TOKEN=your_token,OPENAI_API_KEY=your_key \
  --timeout 540s \
  --memory 512MB</code></pre>

                <h2 id="step-functions">AWS Step Functions (Durable Lambda)</h2>
                <p>For long-running enrichment jobs that exceed Lambda's 15-minute limit:</p>

                <h3>Step Functions State Machine</h3>
                <pre><code class="language-json">{
  "Comment": "Lead Enrichment Pipeline",
  "StartAt": "ReadLeads",
  "States": {
    "ReadLeads": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:us-east-1:123456789012:function:read-leads-from-s3",
      "Next": "EnrichLeads"
    },
    "EnrichLeads": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:us-east-1:123456789012:function:enrich-via-apify",
      "Retry": [
        {
          "ErrorEquals": ["States.TaskFailed"],
          "IntervalSeconds": 30,
          "MaxAttempts": 3,
          "BackoffRate": 2.0
        }
      ],
      "Next": "WriteResults"
    },
    "WriteResults": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:us-east-1:123456789012:function:write-to-s3",
      "End": true
    }
  }
}</code></pre>

                <h3>Enrich Lambda Function</h3>
                <pre><code class="language-python">import boto3
import requests
import os

def lambda_handler(event, context):
    """Long-running enrichment via Step Functions."""
    companies = event['companies']

    # Start async Apify run
    run = requests.post(
        "https://api.apify.com/v2/acts/datahq~airtable-lead-enricher/runs",
        params={"token": os.environ['APIFY_TOKEN']},
        json={
            "mode": "api",
            "companies": companies,
            "enrichment": {"sources": ["google_maps", "website", "hunter"]},
            "llm": {"enabled": True, "provider": "openai", "apiKey": os.environ['OPENAI_KEY']}
        }
    ).json()

    run_id = run['data']['id']

    # Poll for completion (Step Functions handles timeout)
    while True:
        status = requests.get(
            f"https://api.apify.com/v2/actor-runs/{run_id}",
            params={"token": os.environ['APIFY_TOKEN']}
        ).json()

        if status['data']['status'] in ['SUCCEEDED', 'FAILED']:
            break

    # Get results
    dataset_id = status['data']['defaultDatasetId']
    results = requests.get(
        f"https://api.apify.com/v2/datasets/{dataset_id}/items",
        params={"token": os.environ['APIFY_TOKEN']}
    ).json()

    return {"enrichedLeads": results, "count": len(results)}</code></pre>

                <h2 id="snowflake">Snowflake Integration</h2>
                <p>Enrich leads directly from Snowflake tables using Python stored procedures:</p>

                <pre><code class="language-sql">-- Create stored procedure
CREATE OR REPLACE PROCEDURE enrich_leads()
RETURNS STRING
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python', 'requests')
HANDLER = 'enrich_leads_handler'
AS
$$
import requests
import json

def enrich_leads_handler(session):
    # Read pending leads from table
    leads_df = session.table("RAW_LEADS").filter("ENRICHED_AT IS NULL").limit(100)
    companies = []

    for row in leads_df.collect():
        companies.append({
            "companyName": row.COMPANY_NAME,
            "website": row.WEBSITE
        })

    # Enrich via Apify
    response = requests.post(
        "https://api.apify.com/v2/acts/datahq~airtable-lead-enricher/run-sync-get-dataset-items",
        params={"token": "YOUR_APIFY_TOKEN"},
        json={
            "mode": "api",
            "companies": companies,
            "enrichment": {"sources": ["google_maps", "website"]}
        },
        timeout=300
    )

    enriched = response.json()

    # Insert enriched data
    for lead in enriched:
        session.sql(f"""
            UPDATE RAW_LEADS
            SET EMAIL = '{lead.get('email', '')}',
                PHONE = '{lead.get('phone', '')}',
                RATING = {lead.get('rating', 0)},
                LEAD_SCORE = {lead.get('leadScore', 0)},
                ENRICHED_AT = CURRENT_TIMESTAMP()
            WHERE COMPANY_NAME = '{lead['companyName']}'
        """).collect()

    return f"Enriched {len(enriched)} leads"
$$;

-- Schedule with task
CREATE OR REPLACE TASK enrich_leads_daily
  WAREHOUSE = COMPUTE_WH
  SCHEDULE = 'USING CRON 0 2 * * * UTC'
AS
  CALL enrich_leads();

ALTER TASK enrich_leads_daily RESUME;</code></pre>

                <h2 id="redshift">Amazon Redshift Integration</h2>
                <p>Enrich leads from Redshift using Lambda + S3 unload:</p>

                <h3>Step 1: Unload to S3</h3>
                <pre><code class="language-sql">-- Unload pending leads to S3
UNLOAD (
  'SELECT company_name, website
   FROM leads
   WHERE enriched_at IS NULL
   LIMIT 1000'
)
TO 's3://my-bucket/leads/pending_'
IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3Role'
FORMAT AS JSON
PARALLEL OFF;</code></pre>

                <h3>Step 2: Lambda Enrichment</h3>
                <pre><code class="language-python">import boto3
import requests
import json
import psycopg2

def lambda_handler(event, context):
    s3 = boto3.client('s3')

    # Read from S3
    obj = s3.get_object(Bucket='my-bucket', Key='leads/pending_0000')
    companies = [json.loads(line) for line in obj['Body'].read().decode().splitlines()]

    # Enrich
    response = requests.post(
        "https://api.apify.com/v2/acts/datahq~airtable-lead-enricher/run-sync-get-dataset-items",
        params={"token": os.environ['APIFY_TOKEN']},
        json={"mode": "api", "companies": companies},
        timeout=300
    )
    enriched = response.json()

    # Write back to S3
    s3.put_object(
        Bucket='my-bucket',
        Key='leads/enriched.json',
        Body=json.dumps(enriched)
    )

    # Copy to Redshift
    conn = psycopg2.connect(
        host=os.environ['REDSHIFT_HOST'],
        port=5439,
        dbname='analytics',
        user=os.environ['REDSHIFT_USER'],
        password=os.environ['REDSHIFT_PASSWORD']
    )

    with conn.cursor() as cur:
        cur.execute(f"""
            COPY enriched_leads
            FROM 's3://my-bucket/leads/enriched.json'
            IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3Role'
            FORMAT AS JSON 'auto';
        """)
        conn.commit()

    return {"enriched": len(enriched)}</code></pre>

                <h2 id="data-lake">Data Lake Pattern (S3)</h2>
                <p>Write enriched data to partitioned data lakes for analytics:</p>

                <pre><code class="language-python"># Write enriched data to partitioned data lake
from datetime import datetime

partition_path = f"s3://datalake/enriched-leads/year={datetime.now().year}/month={datetime.now().month:02d}/"

s3.put_object(
    Bucket='datalake',
    Key=f"{partition_path}leads_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet",
    Body=df.to_parquet()
)</code></pre>

                <h2 id="rate-limits">Rate Limits & Batching</h2>
                <p>Understanding limits and how to handle large-scale enrichment:</p>

                <table>
                    <thead>
                        <tr>
                            <th>Limit</th>
                            <th>Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Companies/run</td>
                            <td>100</td>
                        </tr>
                        <tr>
                            <td>Concurrent runs</td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td>Run timeout</td>
                            <td>1 hour</td>
                        </tr>
                    </tbody>
                </table>

                <h3>Batch Processing (1000+ leads)</h3>
                <pre><code class="language-python">import asyncio
import aiohttp

async def enrich_all(companies, token):
    """Enrich all companies with controlled concurrency."""
    batches = [companies[i:i+100] for i in range(0, len(companies), 100)]

    async with aiohttp.ClientSession() as session:
        tasks = [enrich_batch(session, batch, token) for batch in batches]
        results = await asyncio.gather(*tasks)

    return [item for batch_results in results for item in batch_results]</code></pre>

                <h2 id="cost-estimation">Cost Estimation</h2>
                <p>Estimated costs for different batch sizes:</p>

                <table>
                    <thead>
                        <tr>
                            <th>Companies</th>
                            <th>Apify Cost</th>
                            <th>LLM Cost</th>
                            <th>Total</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>100</td>
                            <td>$3.00</td>
                            <td>$0.10</td>
                            <td>$3.10</td>
                        </tr>
                        <tr>
                            <td>1,000</td>
                            <td>$30.00</td>
                            <td>$1.00</td>
                            <td>$31.00</td>
                        </tr>
                        <tr>
                            <td>10,000</td>
                            <td>$300.00</td>
                            <td>$10.00</td>
                            <td>$310.00</td>
                        </tr>
                    </tbody>
                </table>

                <div class="info-box">
                    <strong>Note:</strong> LLM costs based on GPT-4o-mini (~$0.001/lead). Hunter.io costs separate if enabled.
                </div>

                <hr style="margin: 40px 0; border: none; border-top: 1px solid #ddd;">

                <p style="text-align: center; color: #666; font-size: 0.9em;">
                    <a href="index.html">Documentation</a> •
                    <a href="extension.html">Extension Guide</a> •
                    <a href="terms.html">Terms</a> •
                    <a href="privacy.html">Privacy</a>
                </p>
            </div>
        </main>
    </div>
</body>
</html>
